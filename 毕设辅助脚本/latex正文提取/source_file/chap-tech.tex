
\chapter{相关技术和理论}
\label{chap:technology}

接下来介绍以下本文所涉及到的相关技术和理论知识，主要是搜索平台框架~Lucene~、针对中文的分词器、网页排序的算法介绍等。

\section{Lucene~简介}

\subsection{Lucene~介绍}

Lucene~是~Apache~Software~Foundation~的一个免费信息检索软件库\cite{lucene_introduce}。Lucene~提供了索引引擎以及查询引擎，以便支持全文检索功能。它使用了高度优化的倒排索引结构，并支持增量索引\cite{lucene_introduce2}，具有性能高、可扩展等特点。整个~Lucene~的系统结构可以用下图 \ref{fig:lucene_system} 表示：

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.7\textwidth]{figures/chap2/chap-2-system_lucene.png}
        \vspace{-1em}
        \caption{系统结构}
        \label{fig:lucene_system}
    \end{figure}

可以看到，其中主要分为三大部分，分别是：

    \begin{itemize}
      \item \textbf{各种供外部使用的~API~}：开发人员调用这些~API~可以进行搜索、分析，以及进一步对搜索结果进行处理等。
      \item \textbf{基本包装结构}：主要是指内部使用的各种数据结构的封装，比如说每一个网页被封装成一个~document~数据结构等。
      \item \textbf{索引核心}：主要提供为数据源建立特定的数据结构，即索引，这是~Lucene~优异检索性能的来源。生成的索引数据要在搜索时提供给对应接口，所以还涉及到存储相关的操作。
    \end{itemize}

\subsection{Lucene~的索引机制}

~Lucene~之所以检索效率高，得益于它对索引结构进行了许多优化。

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.7\textwidth]{figures/chap2/chap2-lucene-index-structure.png}
        \vspace{-1em}
        \caption{Lucene~索引结构}
        \label{fig:lucene_index_structure}
    \end{figure}

常用的索引技术（倒排、后缀数组、签名）中，Lucene~采用倒排索引结构\cite{lucene3}，如图 \ref{fig:lucene_index_structure} 所示，索引文件格式不依赖于操作系统平台，由五层结构组成：

\begin{itemize}
  \item \textbf{索引（Index）}：集成所有索引信息的文件。
  \item \textbf{段（Segment）}：包含~Field~集合和~Term~集合的结构，可以被独立检索。
  \item \textbf{文档（Document）}：由数据源提取后的信息构成，比如提取一个网页，一个网页对应一个文档，包括了网页正文以及~URL~等信息。
  \item \textbf{字段（Field）}：一份文档有多个字段，比如标题字段，内容字段等。
  \item \textbf{术语（Term）}：索引结构的最小单位，提供字符串以及其对应的位置、频率等信息。
\end{itemize}

Lucene~通过文本提取、解析，索引存储来建立索引，如下图 \ref{fig:lucene_index_create_process} 所示，

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=\textwidth]{figures/chap2/chap2-create-index-process.png}
        \vspace{-1em}
        \caption{Lucene~索引建立}
        \label{fig:lucene_index_create_process}
    \end{figure}

此外，Lucene~还采用了许多规则对索引存储进行了优化，包括前缀后缀规则、差值规则、或然跟随规则、跳跃表规则等\cite{lucene_index_4}。


\subsection{Lucene~的检索机制}

基于~Lucene~建立过后的索引库，整个~Lucene~的检索机制大致如下图 \ref{fig:lucene_index_search_process} 所示。Lucene~接受到用户的查询时，在解析完查询请求之后就进入到索引库里进行相应的检索，并进行打分、过滤等操作，之后采用缓存机制读取一定数量的结果返回给用户。

    \begin{figure}[htbp]
        % 输入查询条件 -> 查询解析器，解析查询条件 -> 语言解析器进行切词 -> 检索事先建立好的索引库，得到查询结果。
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.4\textwidth]{figures/chap2/chap2-lucene-search.png}
        \vspace{-1em}
        \caption{Lucene~搜索过程}
        \label{fig:lucene_index_search_process}
    \end{figure}

Lucene~的检索模型基于向量空间模型\cite{lucene_search_5}，实现了一个相关性评价数据检索模型。从用户接收过来的模糊查询，在查询词解析阶段会进行关键字拆分，针对特定语言执行分词操作，从而支持术语（term）匹配操作，可以利用索引进行精确查询并进行关键字间的逻辑运算，最后将搜索结果与查询词以向量形式进行相似度计算作为相关度排序依据。

为了提高检索效率以及减少响应时间，在用户首次检索时，Lucene~并不把所有的检索结果读取出来，而是依照评分规则及相关配置，将其中评分最高的~N~个结果放到缓存中返回给开发者，开发者再从中选取一定数量的结果展示给用户。当用户发出请求期望读取这~N~个之后的检索结果时，Lucene~将会再次执行检索操作，并且读取比上一次大~1~倍的结果返回。

另外，Lucene~在检索的过程中，会自动过滤掉评分低的检索结果，这对开发者来说是透明的。开发者得到检索的结果后，需要根据对应的文档（Document）ID~号读取对应字段的内容，同时利用~Lucene~提供的相关~API~进行结果处理，比如高亮关键词等。至此~Lucene~框架的搜索过程结束，之后就是借由开发者自行扩展的~Web~应用程序将搜索结果展示给用户等的搜索平台操作。

\section{中文分词}

% TODO: 概述一下中文分词的背景啥的
在~Lucene~建立索引以及检索的过程中都涉及到了分词的问题，Lucene~结合英文（单词与单词间用明显的分隔符区分开，比如空格等）的特点，提供了许多分词器。但由于中文词语之间的分隔并没有英文那么明显，想要正确解析中文语句就需要寻找一个合适的中文分词器。

\subsection{中文分词的研究现状}

% TODO: 讲一下分词的流程、有啥算法之类的
目前，常用的分词方法可以被分为~3~大类：基于字符串匹配的分词法、基于统计学的分词法、机器学习分词法\cite{chinese_segmentation_1}。三种方法各有其应用场景，其主要算法及其优劣如下所述：

\begin{itemize}
  \item \textbf{基于字符串匹配的分词法}：亦称为机械分词或词典分词，按照一定的匹配规则对字符串进行扫描、匹配后进行切割，实现简单，但分词准确度不够；
  \item \textbf{基于统计学的分词法}：包括期望最大值算法、变长分词方法等，能够有效识别歧义与新词等，但需要大量训练；
  \item \textbf{机器学习分词法}：机器学习分词法主要有专家系统分词法和神经网络分词法等\cite{lucene_index_4}，可以智能学习，但实现难度较大。
\end{itemize}

针对第一种分词法，衍生出了许多歧义识别法（双向扫描法、逐词扫描法、回退一字法）和歧义消除法（规则性、概率型消解算法）等。整体的分词流程如下图 \ref{fig:string_segmentation} 所示：

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=\textwidth]{figures/chap2/chap2-string-segmentation.png}
        \vspace{-1em}
        \caption{基于字符串匹配的分词法}
        \label{fig:string_segmentation}
    \end{figure}

在实际实现中，大部分分词器并没有实现智能处理模块。另外在匹配的过程中，很多分词器都是基于词典进行匹配的，匹配的结果依赖于词典的好坏。

\subsection{结合~Lucene~实现中文分词}

% TODO: 重点介绍一下 Lucene 有关的中文分词
考虑到实现难度，本文决定使用基于字符串匹配的分词法来进行中文分词。目前有许多现有的分词器，其性能和分词效果都有所差异。大致来说，有这么三种分词器：单字分词、二元分词、词库分词。

这些分词器已经被封装成了对应的类，其类名及其分词特点如下：

\begin{itemize}
  \item \textbf{单字分词}：也称一元分词，按照中文字符一个字一个字地切割分词，非中文字符依照分隔符进行分割，分词器有：StandardAnalyzer、ChineseAnalyzer；
  \item \textbf{二元分词}：一个词包括两个字，即每个字都和前面的一个字及后面的一个字组成一个词，分词器有：CJKAnalyzer；
  \item \textbf{词库分词}：基于给定的词典，按照一定规则进行匹配分词，分词器有：Paoding 分词、极易分词、IkAnalyzer、MMAnalyzer等。
\end{itemize}

不同的分词器的扫描、匹配规则，以及词库依赖性均不同，有必要对这些分词器进行评估，主要的指标是分词结果数量以及耗时开销。对比结果 \cite{chinese_segmentation_2} 如下：

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=\textwidth]{figures/chap2/chap2-analyzer-analysis.png}
        \vspace{-1em}
        \caption{基于字符串匹配的分词法}
        \label{fig:segmentation_analysis}
    \end{figure}

上图 \ref{fig:segmentation_analysis} 通过~4~个指标来进行对比，其对比结果经过归一化处理。好的分词器应该具有如下的特点：检索结果数多、检索耗时低、索引建立占用空间小、索引建立耗时少。但根据对比结果，发现各个分词器各有其优势，比如单字分词在索引建立方面性能突出，但检索效果一般；而~IK~分词检索效果虽好，索引建立却不理想。综合~4~个指标来看，Paoding~分词是这几种分词器里面整体性能最好的。

\section{网页排序算法简介}

在~Lucene~框架完成检索后，默认是按照相关度进行排序的，为了提供给用户更适合的结果，需要对搜索结果再次进行排序。

目前，网页排序算法有如下几种分类\cite{web_rank_algorithm}：

\begin{itemize}
  \item \textbf{基于传统~IR~的内容分析排序}：主要思想是根据查询关键字和网页内容之间的相关性来进行排序，包括有：基于多个子串测试的全文搜索、反转文件、签名文件、矢量模型和聚类等算法。虽然简单易用，但是过度依赖词汇。
  \item \textbf{基于发布者信息的排序}：根据网站开发人员所提供的相关信息进行分析排序，比如总结、分类、链接关系等，包括有基于超链接分析的~Co-Citation、Coupling~算法等。考虑到了信息质量对排名的影响，但存在主题漂移等缺点。
  \item \textbf{基于用户信息的排序}：基于用户反馈，如点击数据，来识别用户查询类型，从而建立映射关系，自动标记相关结果。目前提出建立了许多模型，例如网络搜索用户行为模型~\cite{web_rank_algorithm_1}~等。信息查询能力强，但导航查询能力弱。
  \item \textbf{基于标注信息的排序}：在用户提交查询的同时，根据查询词分析查询方向补充相关标注信息。包括有结果重排和查询扩展等技术。
\end{itemize}

在网站发布者提供的信息当中，其中一个比较受关注的资源就是网页之间的链接关系。当人们评价搜索结果好坏的时候，一个重要的指标就是权威性，比如搜索“苹果”时，如果苹果官网这个权威网页排在搜索结果的前列，则人们会觉得搜索结果排序恰当。而网页链接关系就可以作为计算权威性的数据源，当网页~p~指向了网页~q~的时候，隐含地表明网页~p~的相关人员，认为网页~q~在某个方面具有一定的参考价值（权威性）。因此可以将这种超链接视作~p~对~q~的一种认可，收集这些认可信息，就能够衡量网页的权威性。

本文研究的就是基于超链接分析的网页排序算法，虽然说网页超链关系提供了认可信息，但也存在着干扰因素。在现实生活中，链接关系有复杂的含义，比如说竞争对手之间，就算是互相认可也基本不会为对方创建链接关系；还有经济交易建立的链接关系，比如广告信息，会以链接方式出现在某个网站中，但这并不代表这个网站就认可了广告页面。在采用链接关系作为权威性判断源时，这些干扰因素都应该被考虑在内。于是研究人员们基于网页超链关系提出了许多的网页排序算法以及相关的改进思路，就是为了减少这些干扰带来的影响。

基于超链分析的网页排序算法中，两个最经典算法是~HITS~以及~PageRank~，下面分别进行介绍。

\subsection{HITS~算法介绍}

~HITS~算法是由~Jon M. Kleinberg~于~1999~年发表的论文中提到的。论文里指出，用户一般会进行三种查询：

\begin{itemize}
  \item \textbf{窄主题检索提问}：比如说，“Netscape 是否支持 JDK1.1 版本的 code-signing API ？”
  \item \textbf{宽主题检索提问}：比如说，“查找有关 Java 编程语言的信息”
  \item \textbf{相似网页检索提问}：比如说，“查找与 java.sun.com 相似的网页”
\end{itemize}

~HITS~算法主要是针对宽主题检索的，宽主题检索的条件下，用户能够搜索到过于冗余的搜索结果，于是想要有一种方法能够进行过滤，找出其中最具有权威性和准确性的网页。

而论文作者提出了一个基于链接的权威性模型。模型基于权威值和中心值的概念进行计算，能够识别中心页面和权威页面。通过构建万维网的焦点子图，并使用链接关系进行迭代计算出集合中每个网页的中心值和权威值，最终能够实现识别这两种页面，提供过滤搜索结果的功能。

\subsubsection{构建万维网的焦点子图}

作者假设针对某个宽主题检索提问，会有一部分中心页面，这部分页面会指向权威页面。于是作者考虑构建一个关于万维网宽主题检索提问的聚焦子图的方法，从局部角度触发，产生一组具有一定权威性的相关候选页面。

构造的算法如下所示：

\lstinputlisting{code/hits_pseudo_code_1.java}

算法使用了根集 R 来产生这组权威性相关候选页面，而根集 R 是有普通的文本搜索引擎得到的按相关度排序的结果中的前 t 个。针对每个根集 R 中的页面，我们将该页面指向的所有页面都加入进候选页面集合中，而将指向该页面的p个页面中选取一部分加入到候选页面中（只选取一部分是为了保证最后构造出来的~S~（基本集）较小），最终遍历完根集页面就是我们构造好的焦点子图。

算法的核心思想是，考虑某个查询的一个权威性页面 ―― 虽然它可能不在根集~R~中，但它很可能被~R~中至少一个页面所链接。因此通过链接关系扩展根集从而得到基本集~S，从而使得~S~能够包含大多数主题相关的权威页面，如下图~\ref{fig:root2base}~所示：

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.7\textwidth]{figures/chap2/chap2-root2base.png}
        \vspace{-1em}
        \caption{根集-$>$基本集}
        \label{fig:root2base}
    \end{figure}

% TODO：这里还有一个步骤，要在基本集中删除所有内联链接（即指向自身的链接），这个我自己的代码还没实现这一步，只保留横向链接的话，意味着得去构造了吧，搜狗数据源所提供的链接数据并不可取。
这样做还有另外一个好处：原本的根集~R~中，之间通常极少有链接关系，这使得它基本上是“无结构的”，通过扩展得到的基本集将具有相对多的链接关系。作为算法的优化，这里还进行了一步操作，减少导航链接对算法的影响，比如基本集 S 删除所有内联链接（自己指向自己），只保留横向链接（指向别人）。

\subsubsection{计算中心值和权威值}

由上面的步骤可以计算得到一个包含权威页面以及富有链接关系的基本集~S~。现在的目标是要从中找出中心页面以及权威页面。HITS~算法通过分析链接关系来得到这两组页面。

这里~HITS~算法做了一个假设，假设一个权威页面应该被许多中心页面所指向，而一个中心页面应该指向许多权威页面。

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.7\textwidth]{figures/chap2/chap2-hits-suppose.png}
        \vspace{-1em}
        \caption{~HITS~算法假设}
        \label{fig:hits_algorithm_suppose}
    \end{figure}

    利用这种中心值和权威值的关系，通过一个迭代算法，维护和更新每个页面的中心值和权威值，当迭代算法收敛的时候，就能够识别基本集~S~中的中心页面和权威页面，从而解决这种循环。

    迭代算法如下所示：

    \lstinputlisting{code/hits_pseudo_code_recursion.java}

    其中，迭代算法中所进行的~I~O~操作，如图~\ref{fig:hits_recursion_algorithm_base_operation}~所示：

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.3\textwidth]{figures/chap2/chap2-recursion-algorithm.png}
        \vspace{-1em}
        \caption{迭代算法的基本操作}
        \label{fig:hits_recursion_algorithm_base_operation}
    \end{figure}

    论文~\cite{hits_base}~验证指出当~k~取值~20~时，这个迭代过程可以收敛，即每个页面的中心值和权威值都稳定在一个范围里。再依据这个中心值和权威值进行排序，选出其中中心值最高的几个页面以及权威值最高的几个页面，分别构成中心页面集和权威页面集。

\subsection{PageRank~算法介绍}

PageRank~算法\cite{pagerank_base}也是基于网页之间的超链接关系进行计算的，其核心思想是，当某个网页~p~被其他网页~q~引用时，说明~q~认为网页~p~是重要的。因此网页~p~的重要性就可以由所有引用了它的网页的总数来进行计算。

与~HITS~算法不同的是，PageRank~算法为每个网页维护一个~PR~值而不是中心值及权威值。~PageRank~算法会根据每个网页的~PR~值来进行评价，~PR~值较高的网页往往被认为“重要性”更高的网页，因此排序时这个网页也会靠前。

    \begin{figure}[htbp]
        \centering
        \numberwithin{figure}{chapter}
        \includegraphics[width=0.7\textwidth]{figures/chap2/chap2-pagerank.png}  
        \vspace{-1em}
        \caption{PageRank~算法}
        \label{fig:pagerank_demo}
    \end{figure}

首先得对每个网页的~PR~值进行初始化，然后按照如下公式进行计算：
\begin{displaymath}
PR(p_i) = \frac{1 - d}{N} + d * \sum_{p_j \in M(p_i)} \frac{PR(p_j)}{L(p_j)}
\end{displaymath}
其中的各个参数及其含义：

\begin{itemize}
  \item \textbf{$PR$}：PR~值，即~PageRank~算法中用来表示网页重要程度的符号。
  \item \textbf{$d$}：阻尼系数，表示当用户看到某个链接时会点击这个链接进入到对应网页的概率。这个阻尼因素主要是用来处理内联链接等。
  \item \textbf{$N$}：参与计算的所有网页数目。
  \item \textbf{$p_i$}：表示网页，比如~$p_1$、$p_2$、...、$p_N$~各自代表着一个网页。
  \item \textbf{$M(p_i)$}：表示一组集合，该集合中的每个网页都含有指向~$p_i$~的链接。
  \item \textbf{$L(p_j)$}：指的是~$p_j$~这个网页里面存在的所有链接总数。
\end{itemize}

经过~$k$~轮迭代后可以达到收敛，此时的各个网页的重要性将各自维持在一个较小范围的值域内，因此就可以依据所计算得到的重要性来进行排序了。

\subsection{小结}

经典的~HITS~和~PageRank~算法都是~1999~年提出的，在实际运用过程中体现了其优势所在，但也暴露出了许多不足之处，研究人员们也因此不断进行改进创新。

~HITS~与~PageRank~算法都是基于超链接结构来进行的，没有评估网页内容的相关性，因此或多或少都存在着主题漂移的问题，即排序过后的结果可能是跟查询主题不相关的；另外由于新发布的网页短期内不会被大量链接所指向，所以存在着旧网页排名靠前的问题。PageRank~作为一个静态的算法，可以离线计算所有页面的~PageRank~，相比之下~HITS~算法需要实时进行计算，导致检索耗时会比~PageRank~大。但~PageRank~只考虑了链接的数目，没有考虑链接页面的中心值，而~HITS~同时对中心值和权威值来进行综合考量。在收敛速度方面，~PageRank~考虑的是整个链接结构而~HITS~只考虑与主题相关的一个相对较小集合里面的链接结构，因此~HITS~算法的收敛是要快于~PageRank~的。

对于这些不足之处，论文\cite{pagerank_improve}基于时间反馈和主题相似性对~PageRank~进行了改进，减少了主题漂移现象同时考虑到了新旧网页的排名；论文\cite{pagerank_improve_2}除了考虑~PR~值还考虑了另一个称为相关分数的值，尝试解决搜索歧义的问题；论文\cite{pagerank_improve_3}提出基于比例的加权~PageRank~，加快了~PageRank~算法的收敛速度。以上是针对~PageRank~算法的改进，但是其解决的问题是~HITS~和~PageRank~等基于超链接的算法都存在的共性问题，对除了~PageRank~以外的算法也存在着参考价值。

针对~HITS~算法存在的主题漂移以及相关性问题，论文\cite{ieee_hits_improve_1}通过考虑内容的相似性因素来解决；由于~HITS~是动态实时检索，论文\cite{ieee_hits_improve_2}将~HITS~算法运用在实时流媒体服务上，用于给使用者推荐合适的频道；另外，论文\cite{ieee_hits_improve_3}研究探讨~HITS~算法对大量网页进行排名所需要的处理器数量，通过分析共享内存超级计算机并在整个南密西西比大学网络中进行结果验证得到对数关系图。